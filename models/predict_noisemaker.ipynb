{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv   \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import librosa as lr\n",
    "import librosa.display as lrd\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_TRAIN_TEST = .8\n",
    "TARGET_FILE = '../data/noisemaker/noisemaker.csv'\n",
    "SAMPLES_FOLDER = '../data/noisemaker/noisemaker_samples/'\n",
    "FOLDER_PREFIX = 'noisemaker'\n",
    "\n",
    "D = str(datetime.datetime.now())\n",
    "RESULTS_FILE = 'RESULTS-' + FOLDER_PREFIX + '-' + D[:16].replace(' ','_') + '.csv'\n",
    "print RESULTS_FILE\n",
    "\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "N_MFCC = 20\n",
    "N_MEL = 32\n",
    "\n",
    "SUBSET = False\n",
    "N_EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 1\n",
    "VALIDATION = .1\n",
    "\n",
    "DEPENDENCIES = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepares datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares labels\n",
    "Y = pd.read_csv(TARGET_FILE)\n",
    "if SUBSET:\n",
    "    Y = Y.head(100)\n",
    "print Y.head()\n",
    "\n",
    "len_train = int(Y.shape[0]*.8)\n",
    "len_test = int(Y.shape[0]*.2)\n",
    "print len_train, len_test\n",
    "\n",
    "Y_train = Y.iloc[:len_train,:]\n",
    "Y_test = Y.iloc[len_train:,:]\n",
    "print \n",
    "print 'Training data:', Y_train.shape\n",
    "print 'Test data:', Y_test.shape\n",
    "\n",
    "Y_train, files_train =  Y_train.drop('file', axis=1), Y_train['file']\n",
    "Y_test, files_test = Y_test.drop('file', axis=1), Y_test['file']\n",
    "print\n",
    "print files_train[:5]\n",
    "print files_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads waveforms\n",
    "sampling_rate = lr.load(SAMPLES_FOLDER + files_train[0])[1]\n",
    "print 'Sampling rate:', sampling_rate\n",
    "\n",
    "waveforms_test = []\n",
    "for i, file_name in enumerate(files_test):\n",
    "    if i % 100 == 0:\n",
    "        print 'Loaded testing waveform #' + str(i)\n",
    "    f = SAMPLES_FOLDER + file_name\n",
    "    waveforms_test.append(lr.load(f)[0])\n",
    "X_test = np.stack(waveforms_test)\n",
    "print X_test.shape\n",
    "\n",
    "\n",
    "waveforms_train = []\n",
    "for i, file_name in enumerate(files_train):\n",
    "    if i % 100 == 0:\n",
    "        print 'Loaded training waveform #' + str(i)\n",
    "    f = SAMPLES_FOLDER + file_name\n",
    "    waveforms_train.append(lr.load(f)[0])\n",
    "X_train = np.stack(waveforms_train)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepares the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test_mfcc = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    S = X_test[i,:]\n",
    "    mfcc = lr.feature.mfcc(y=S, sr=sampling_rate, hop_length=HOP_LENGTH, n_mfcc=N_MFCC)\n",
    "    L_test_mfcc.append(mfcc)\n",
    "X_test_mfcc = np.stack(L_test_mfcc)\n",
    "print X_test_mfcc.shape\n",
    "\n",
    "L_train_mfcc = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    S = X_train[i,:]\n",
    "    mfcc = lr.feature.mfcc(y=S, sr=sampling_rate, hop_length=HOP_LENGTH, n_mfcc=N_MFCC)\n",
    "    L_train_mfcc.append(mfcc)\n",
    "X_train_mfcc = np.stack(L_train_mfcc)\n",
    "print X_train_mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "lr.display.specshow(X_train_mfcc[10,...], x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mfcc = np.reshape(X_test_mfcc, (X_test_mfcc.shape[0],-1))\n",
    "X_train_mfcc = np.reshape(X_train_mfcc, (X_train_mfcc.shape[0],-1))\n",
    "print X_test_mfcc.shape\n",
    "print X_train_mfcc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "headers=['algo', 'type', 'param1','param2','regularization',\n",
    "         'target','metric', 'train_score', 'test_score']\n",
    "with open(RESULTS_FILE, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    \n",
    "def evaluate_algo(algo_label, grid_reg, grid_class):\n",
    "    results = []\n",
    "    for t_i, t_name in enumerate(list(Y_train)):\n",
    "\n",
    "        print 'predicting feature', t_name\n",
    "        y_train = Y_train[t_name]\n",
    "        y_test = Y_test[t_name]\n",
    "        print y_train.dtype\n",
    "        \n",
    "        if t_name in DEPENDENCIES:\n",
    "            switch = DEPENDENCIES[t_name]\n",
    "            \n",
    "            to_keep_train = (Y_train[switch] == 'on')\n",
    "            print 'Keeping', str(sum(to_keep_train)), 'training examples out of',  str(len(to_keep_train))\n",
    "            y_train = y_train[to_keep_train]\n",
    "            X_train= X_train_mfcc[to_keep_train,...]\n",
    "            \n",
    "            to_keep_test = (Y_test[switch] == 'on')\n",
    "            print 'Keeping', str(sum(to_keep_test)), 'training examples out of',  str(len(to_keep_test))\n",
    "            y_test = y_test[to_keep_test]\n",
    "            X_test = X_test_mfcc[to_keep_test,...]\n",
    "        \n",
    "        else:\n",
    "            X_train= X_train_mfcc\n",
    "            X_test= X_test_mfcc\n",
    "        \n",
    "        print X_train.shape, y_train.shape\n",
    "        print X_test.shape, y_test.shape\n",
    "        \n",
    "        best_params_C = None\n",
    "        best_params_R = None\n",
    "        \n",
    "        try:\n",
    "            # Case 1: classification\n",
    "            if y_train.dtype == 'object':\n",
    "\n",
    "                metric = 'class'\n",
    "\n",
    "                # Cross-Validation Score\n",
    "                grid_class.fit(X_train, y_train)\n",
    "                cross_val_scores = grid_class.best_score_\n",
    "                best_params_C = grid_class.best_params_\n",
    "                print cross_val_scores\n",
    "\n",
    "                # Test Score\n",
    "                test_pred = grid_class.predict(X_test)\n",
    "                test_score = accuracy_score(test_pred, y_test)\n",
    "                print test_score\n",
    "\n",
    "\n",
    "            # Case 2: regression\n",
    "            elif y_train.dtype == 'float64':\n",
    "\n",
    "                metric = 'reg'\n",
    "\n",
    "                # Cross-Validation Score\n",
    "                grid_reg.fit(X_train, y_train)\n",
    "                cross_val_scores = grid_reg.best_score_ * -1\n",
    "                best_params_R = grid_reg.best_params_\n",
    "                print cross_val_scores\n",
    "\n",
    "                # Test Score\n",
    "                test_pred = grid_reg.predict(X_test)\n",
    "                test_score = mean_absolute_error(test_pred, y_test)\n",
    "                print test_score\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Wrong Column Type')\n",
    "\n",
    "            out = (algo_label, 'per_output', best_params_R, best_params_C,None,\n",
    "                   t_name, metric, cross_val_scores, test_score)\n",
    "            print out\n",
    "            with open(RESULTS_FILE, 'a') as f:\n",
    "                writer = csv.writer(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                writer.writerow(out)\n",
    "\n",
    "        except:\n",
    "            print \"Unexpected error:\", sys.exc_info()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "algo_C =neighbors.KNeighborsClassifier()\n",
    "grid_C = GridSearchCV(algo_C, {\"n_neighbors\":[3,5,8,16]}, scoring='accuracy', verbose=2)\n",
    "\n",
    "algo_R =neighbors.KNeighborsRegressor()\n",
    "grid_R = GridSearchCV(algo_R, {\"n_neighbors\":[3,5,8,16]}, scoring='neg_mean_absolute_error', verbose=2)\n",
    "\n",
    "evaluate_algo('kNN', grid_R, grid_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "algo_C = tree.DecisionTreeClassifier()\n",
    "grid_C = GridSearchCV(algo_C, {\"max_depth\":[2,4,8,16,32,64,128]}, scoring='accuracy', verbose=2)\n",
    "\n",
    "algo_R = tree.DecisionTreeRegressor()\n",
    "grid_R = GridSearchCV(algo_R, {\"max_depth\":[2,4,8,16,32,64,128]}, scoring='neg_mean_absolute_error', verbose=2)\n",
    "\n",
    "evaluate_algo('Decision Tree', grid_R, grid_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import dummy\n",
    "\n",
    "algo_C = dummy.DummyClassifier()\n",
    "grid_C = GridSearchCV(algo_C, {\"strategy\":[\"most_frequent\"]}, scoring='accuracy', verbose=2)\n",
    "\n",
    "algo_R = dummy.DummyRegressor()\n",
    "grid_R = GridSearchCV(algo_R, {\"strategy\":[\"mean\"]}, scoring='neg_mean_absolute_error', verbose=2)\n",
    "\n",
    "evaluate_algo('Naive', grid_R, grid_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def target_info(df_y):\n",
    "    infos = collections.OrderedDict()\n",
    "    \n",
    "    for i,name in enumerate(list(df_y)):\n",
    "        print name\n",
    "        \n",
    "        y = df_y[[name]].values.flatten()\n",
    "        infos[name] = {'type':y.dtype}\n",
    "        \n",
    "        if y.dtype=='object':\n",
    "            u_vals = sorted(np.unique(y))\n",
    "            infos[name]['int2char'] = {i:v for i,v in enumerate(u_vals)}\n",
    "            infos[name]['char2int'] = {v:i for i,v in enumerate(u_vals)}\n",
    "            \n",
    "        elif y.dtype=='float64':\n",
    "            infos[name]['mean'] = np.mean(y)\n",
    "            infos[name]['sd'] = np.std(y)\n",
    "        \n",
    "    return infos\n",
    "\n",
    "# Gets target info\n",
    "Y_info = target_info(Y_train)\n",
    "print Y_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preps input data\n",
    "X_train_mfcc = X_train_mfcc.reshape([X_train_mfcc.shape[0],N_MFCC,-1]).transpose(0,2,1)\n",
    "X_test_mfcc = X_test_mfcc.reshape([X_test_mfcc.shape[0],N_MFCC,-1]).transpose(0,2,1)\n",
    "\n",
    "print ''\n",
    "print X_train_mfcc.shape\n",
    "print X_test_mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def format_output(Y):\n",
    "    out = {}\n",
    "    for name,infos in Y_info.iteritems():\n",
    "\n",
    "        # Normalizes/one-hot encodes\n",
    "        if infos['type'] == 'float64':\n",
    "            y = (Y[name] - infos['mean']) / infos['sd']\n",
    "            \n",
    "        elif infos['type'] == 'object':\n",
    "            char2int = infos['char2int']\n",
    "            y = np.zeros((len(Y[name]), len(char2int)))\n",
    "            for i,yval in enumerate(Y[name]):\n",
    "                y[i, char2int[yval]] = 1        \n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        # Sets to zero if necessary\n",
    "        if name in DEPENDENCIES:\n",
    "            switch = DEPENDENCIES[name]\n",
    "            is_off = Y[switch] == \"off\"\n",
    "            print 'Setting', str(sum(is_off)),'elements to zero'\n",
    "            y[is_off] = 0\n",
    "        \n",
    "        print name, y.shape\n",
    "        out[name] = y\n",
    "    \n",
    "    return out\n",
    "\n",
    "        \n",
    "def test_model(label, params1, params2, regul, model, X_train, X_test):\n",
    "    \n",
    "    try:\n",
    "        # Prepares input and output data\n",
    "        X_mean = np.mean(X_train, axis=(0))\n",
    "        X_sd = np.std(X_train, axis=(0))\n",
    "\n",
    "        X_train = (X_train - X_mean) / X_sd\n",
    "        X_test  = (X_test - X_mean) / X_sd\n",
    "\n",
    "        Y_dict_train = format_output(Y_train)\n",
    "        Y_dict_test  = format_output(Y_test)\n",
    "\n",
    "        # Trains the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        hist = model.fit(X_train, Y_dict_train, \n",
    "                  epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE,\n",
    "                 validation_split = VALIDATION, callbacks=[early_stopping])\n",
    "\n",
    "        # Makes predictions\n",
    "        pred = model.predict(X_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "\n",
    "        # Gets validation and test scores (for each metric)\n",
    "        Y = {}\n",
    "        for j, Y_pred in enumerate(pred):\n",
    "            print 'Output:', j\n",
    "            t_name = Y_info.keys()[j]\n",
    "            t_infos = Y_info[t_name]\n",
    "            truth = Y_test[t_name]\n",
    "            print t_name\n",
    "            print Y_pred.shape\n",
    "\n",
    "            if t_name in DEPENDENCIES:\n",
    "                switch = DEPENDENCIES[t_name]\n",
    "\n",
    "                to_keep_test = (Y_test[switch] == 'on')\n",
    "                print 'Keeping', str(sum(to_keep_test)), 'training examples out of',  str(len(to_keep_test))\n",
    "                truth = truth[to_keep_test]\n",
    "                Y_pred = Y_pred[to_keep_test,...]\n",
    "                print Y_pred.shape, truth.shape\n",
    "\n",
    "            if t_infos['type'] == 'float64':\n",
    "                test_metric = 'reg'\n",
    "                Y_pred = Y_pred.flatten()\n",
    "                y_pred = Y_pred * t_infos['sd'] + t_infos['mean']\n",
    "                y_pred = y_pred.tolist()\n",
    "                test_score = mean_absolute_error(y_pred, truth)\n",
    "\n",
    "            elif t_infos['type'] == 'object':\n",
    "                test_metric = 'class'\n",
    "                y_i = np.argmax(Y_pred, axis=1)\n",
    "                y_pred = [t_infos['int2char'][y] for y in y_i]\n",
    "                test_score = accuracy_score(y_pred, truth)\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            train_scores = -1 * hist.history['val_loss'][-1]\n",
    "            out = (label, 'joint', params1, params2, regul,\n",
    "                   t_name, test_metric, train_scores, test_score)\n",
    "            print out\n",
    "            with open(RESULTS_FILE, 'a') as f:\n",
    "                writer = csv.writer(f,quoting=csv.QUOTE_NONNUMERIC)\n",
    "                writer.writerow(out)\n",
    "\n",
    "            Y[t_name] = y_pred\n",
    "            \n",
    "    except:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "        return\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_output(h_layer):\n",
    "    # Output layers and losses\n",
    "    out_layers = []\n",
    "    losses = {}\n",
    "    metrics = {}\n",
    "    \n",
    "    # Adds on/off switches\n",
    "    switches = []\n",
    "    for name,infos in Y_info.iteritems():\n",
    "        if not name in DEPENDENCIES and infos['type'] == 'object':\n",
    "            out_dim = len(infos['char2int'])\n",
    "            out_layer = Dense(out_dim, activation='softmax', name=name)(h_layer)\n",
    "            losses[name] = 'categorical_crossentropy'\n",
    "            metrics[name] = 'accuracy'\n",
    "            switches.append(name)\n",
    "            out_layers.append(out_layer)\n",
    "            \n",
    "        if not name in DEPENDENCIES and infos['type'] == 'float64':\n",
    "            out_layer = Dense(1, name=name)(h_layer)\n",
    "            losses[name] = 'mean_absolute_error'\n",
    "            metrics[name] = 'mean_absolute_error'\n",
    "            switches.append(name)\n",
    "            out_layers.append(out_layer)\n",
    "            \n",
    "    # Adds dependent switches and knobs\n",
    "    dependent = []\n",
    "    for name,infos in Y_info.iteritems():\n",
    "        \n",
    "        if name in DEPENDENCIES and infos['type'] == 'float64':\n",
    "            pred_layer = Dense(1)(h_layer)\n",
    "            \n",
    "            switch = DEPENDENCIES[name]\n",
    "            li_switch = switches.index(switch)\n",
    "            i_on = Y_info[switch]['char2int']['on']\n",
    "            switch = Lambda(lambda T: T[:,i_on])(out_layers[li_switch])\n",
    "            out_layer = Multiply(name = name)([pred_layer, switch])\n",
    "            \n",
    "            losses[name] = 'mean_absolute_error'\n",
    "            metrics[name] = 'mean_absolute_error'\n",
    "            out_layers.append(out_layer)\n",
    "            dependent.append(name)\n",
    "\n",
    "        \n",
    "        if name in DEPENDENCIES and infos['type'] == 'object':\n",
    "            out_dim = len(infos['char2int'])\n",
    "            pred_layer = Dense(out_dim, activation='softmax')(h_layer)            \n",
    "            \n",
    "            switch = DEPENDENCIES[name]\n",
    "            li_switch = switches.index(switch)\n",
    "            i_on = Y_info[switch]['char2int']['on']\n",
    "            switch = Lambda(lambda T: T[:,i_on])(out_layers[li_switch])\n",
    "            out_layer = Multiply(name = name)([pred_layer, switch])\n",
    "            \n",
    "            losses[name] = 'categorical_crossentropy'\n",
    "            metrics[name] = 'accuracy'\n",
    "            out_layers.append(out_layer)\n",
    "            dependent.append(name)\n",
    "        \n",
    "    # Reorders everything\n",
    "    all_out = switches + dependent\n",
    "    ord_out = []\n",
    "    for out_name in Y_info:\n",
    "        o = out_layers[all_out.index(out_name)]\n",
    "        ord_out.append(o)\n",
    "\n",
    "    return ord_out, metrics, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: 1 layer perceptron\n",
    "from keras.layers import Input, Dense, Flatten, Multiply, Lambda, RepeatVector\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def MLP(n_hidden_units, n_layers, regul, in_shape):\n",
    "    # Input layer\n",
    "    in_layer = Input(shape=in_shape)\n",
    "    \n",
    "    # Hidden layers\n",
    "    h_layer = Flatten()(in_layer)\n",
    "    for i in range(n_layers):\n",
    "        h_layer = Dense(n_hidden_units, activation='relu')(h_layer)\n",
    "\n",
    "    out_layers, metrics, losses = NN_output(h_layer)\n",
    "    model = Model(inputs = in_layer, outputs = out_layers)\n",
    "    model.compile(loss=losses, optimizer='adam')\n",
    "    \n",
    "    print model.summary()\n",
    "    return model\n",
    "\n",
    "for N_LAYERS in [1,2]:\n",
    "    for N_UNITS in [64,90,128]:\n",
    "        for REGUL in [0]:\n",
    "            in_dim = X_train_mfcc.shape[1:]\n",
    "            model = MLP(N_UNITS, N_LAYERS, REGUL, in_dim)\n",
    "            out = test_model('MLP', N_UNITS, N_LAYERS, REGUL, model, X_train_mfcc, X_test_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Multiply, Lambda, RepeatVector\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers import LSTM\n",
    "\n",
    "def modLSTM(n_hidden_units, n_layers, regul, in_shape):\n",
    "    # Input layer\n",
    "    in_layer = Input(shape=in_shape)\n",
    "    \n",
    "    # Hidden layers\n",
    "    h_layer = in_layer\n",
    "    for i in range(n_layers):\n",
    "        seq = i < n_layers - 1\n",
    "        h_layer = LSTM(n_hidden_units, return_sequences=seq, dropout=regul)(h_layer)\n",
    "        \n",
    "    # Output layers and losses\n",
    "    out_layers, metrics, losses = NN_output(h_layer)\n",
    "    model = Model(inputs = in_layer, outputs = out_layers)\n",
    "    model.compile(loss=losses, optimizer='adam')\n",
    "    \n",
    "    print model.summary()\n",
    "    return model\n",
    "\n",
    "for N_LAYERS in [1,2]:\n",
    "    for N_UNITS in [32,64,80,128]:\n",
    "        for REGUL in [0,0.2]:\n",
    "            in_dim = X_train_mfcc.shape[1:]\n",
    "            model = modLSTM(N_UNITS, N_LAYERS, REGUL, in_dim)\n",
    "            out = test_model('LSTM', N_UNITS, N_LAYERS, REGUL, model, X_train_mfcc, X_test_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D\n",
    "\n",
    "W = 4\n",
    "\n",
    "def modConvo(n_filters, n_layers, regul, in_shape):\n",
    "    # Input layer\n",
    "    print in_shape\n",
    "    in_layer = Input(shape=in_shape)\n",
    "    \n",
    "    # Hidden layers\n",
    "    h_layer = in_layer\n",
    "    for i in range(n_layers):\n",
    "        print i\n",
    "        conv_layer = Conv1D(n_filters*(i+1), W, \n",
    "                          kernel_regularizer=regularizers.l2(regul))(h_layer)\n",
    "        h_layer = MaxPooling1D(W, strides=2)(conv_layer)\n",
    "    \n",
    "    h_layer_pool = GlobalMaxPooling1D()(h_layer)\n",
    "    h_layer_full = Dense(n_filters, \n",
    "                          kernel_regularizer=regularizers.l2(regul))(h_layer_pool)\n",
    "    \n",
    "    # Output layers and losses\n",
    "    out_layers, metrics, losses = NN_output(h_layer_full)\n",
    "    model = Model(inputs = in_layer, outputs = out_layers)\n",
    "    model.compile(loss=losses, optimizer='adam')\n",
    "    \n",
    "    print model.summary()\n",
    "    return model\n",
    "\n",
    "for N_FILTERS in [16,32,48,64]:\n",
    "    for N_LAYERS in [1,2]:\n",
    "        for REGUL in [0]:\n",
    "            in_dim = X_train_mfcc.shape[1:]\n",
    "            model = modConvo(N_FILTERS, N_LAYERS, REGUL, in_dim)\n",
    "            out = test_model('Conv', N_FILTERS, N_LAYERS, REGUL, model, X_train_mfcc, X_test_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def target_info(df_y):\n",
    "    infos = collections.OrderedDict()\n",
    "    \n",
    "    for i,name in enumerate(list(df_y)):\n",
    "        print name\n",
    "        \n",
    "        y = df_y[[name]].values.flatten()\n",
    "        infos[name] = {'type':y.dtype}\n",
    "        \n",
    "        if y.dtype=='object':\n",
    "            u_vals = sorted(np.unique(y))\n",
    "            infos[name]['int2char'] = {i:v for i,v in enumerate(u_vals)}\n",
    "            infos[name]['char2int'] = {v:i for i,v in enumerate(u_vals)}\n",
    "            \n",
    "        elif y.dtype=='float64':\n",
    "            infos[name]['mean'] = np.mean(y)\n",
    "            infos[name]['sd'] = np.std(y)\n",
    "        \n",
    "    return infos\n",
    "\n",
    "# Gets target info\n",
    "Y_info = target_info(Y_train)\n",
    "print Y_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test_mel = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    S = X_test[i,:]\n",
    "    mel = lr.feature.melspectrogram(y=S, sr=sampling_rate, hop_length=HOP_LENGTH, n_mels=N_MEL)\n",
    "    L_test_mel.append(mel)\n",
    "X_test_mel = np.stack(L_test_mel)\n",
    "print X_test_mel.shape\n",
    "\n",
    "L_train_mel = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    S = X_train[i,:]\n",
    "    mel = lr.feature.melspectrogram(y=S, sr=sampling_rate, hop_length=HOP_LENGTH, n_mels=N_MEL)\n",
    "    L_train_mel.append(mel)\n",
    "X_train_mel = np.stack(L_train_mel)\n",
    "print X_train_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "lr.display.specshow(X_train_mel[1,...], x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mel = np.transpose(X_train_mel, (0,2,1))\n",
    "X_test_mel = np.transpose(X_test_mel, (0,2,1))\n",
    "print X_train_mel.shape, X_test_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N_LAYERS in [1,2]:\n",
    "    for N_UNITS in [64,90,128]:\n",
    "        for REGUL in [0]:\n",
    "            in_dim = X_train_mel.shape[1:]\n",
    "            model = MLP(N_UNITS, N_LAYERS, REGUL, in_dim)\n",
    "            out = test_model('MLP_mid', N_UNITS, N_LAYERS, REGUL, model, X_train_mel, X_test_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N_LAYERS in [1,2]:\n",
    "    for N_UNITS in [32,64,80,128]:\n",
    "        for REGUL in [0,0.2]:\n",
    "            in_dim = X_train_mel.shape[1:]\n",
    "            model = modLSTM(N_UNITS, N_LAYERS, REGUL, in_dim)\n",
    "            out = test_model('LSTM_mid', N_UNITS, N_LAYERS, REGUL, model, X_train_mel, X_test_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N_FILTERS in [16,32,48,64]:\n",
    "    for N_LAYERS in [1,2]:\n",
    "        for REGUL in [0]:\n",
    "            in_dim = X_train_mel.shape[1:]\n",
    "            model = modConvo(N_FILTERS, N_LAYERS, REGUL, in_dim)\n",
    "            out = test_model('Conv_mid', N_FILTERS, N_LAYERS, REGUL, model, X_train_mel, X_test_mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i RESULTS_FILE\n",
    "\n",
    "library(tidyverse)\n",
    "\n",
    "# Processing\n",
    "csv <- read.csv(RESULTS_FILE)\n",
    "\n",
    "indep_models <- csv %>% filter(type == 'per_output')\n",
    "\n",
    "joint_models <- csv %>% filter(type == 'joint')\n",
    "to_keep <- joint_models %>% \n",
    "                group_by(algo, type, param1,param2,regularization) %>%\n",
    "                summarize(train_score = mean(train_score)) %>%\n",
    "                group_by(algo) %>%\n",
    "                filter(train_score == max(train_score)) %>% data.frame()\n",
    "print(to_keep)\n",
    "\n",
    "joint_models <- semi_join(joint_models, to_keep, by=c(\"algo\",\"param1\",\"param1\",\"regularization\")) %>%\n",
    "                distinct()\n",
    "\n",
    "all_models <- rbind(indep_models, joint_models)\n",
    "\n",
    "# Plotting\n",
    "to_plot <- all_models %>%\n",
    "            select(algo, metric, test_score) %>%\n",
    "            group_by(algo, metric) %>%\n",
    "            summarize(score = mean(test_score)) %>%\n",
    "            as.data.frame\n",
    "print(to_plot)\n",
    "\n",
    "to_plot$metric <- factor(to_plot$metric)\n",
    "\n",
    "p <- ggplot(to_plot, aes(x=factor(algo), y=score)) +\n",
    "    geom_bar(stat='identity') +\n",
    "    facet_grid(metric~., scales='free')\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "\n",
    "RESULTS_FILE <- \"RESULTS-noisemaker-2018-01-10_15:47.csv\"\n",
    "\n",
    "library(tidyverse)\n",
    "\n",
    "# Processing\n",
    "csv <- read.csv(RESULTS_FILE)\n",
    "\n",
    "indep_models <- csv %>% filter(type == 'per_output')\n",
    "\n",
    "joint_models <- csv %>% filter(type == 'joint')\n",
    "to_keep <- joint_models %>% \n",
    "                group_by(algo, type, param1,param2,regularization) %>%\n",
    "                summarize(train_score = mean(train_score)) %>%\n",
    "                group_by(algo) %>%\n",
    "                filter(train_score == max(train_score)) %>% data.frame()\n",
    "print(to_keep)\n",
    "\n",
    "joint_models <- semi_join(joint_models, to_keep, by=c(\"algo\",\"param1\",\"param1\",\"regularization\")) %>%\n",
    "                distinct()\n",
    "\n",
    "all_models <- rbind(indep_models, joint_models)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "to_plot <- all_models %>%\n",
    "            select(algo, metric, test_score) %>%\n",
    "            group_by(algo, metric) %>%\n",
    "            summarize(score = mean(test_score)) %>%\n",
    "            as.data.frame\n",
    "to_plot$family <- sapply(to_plot$algo,function(n){\n",
    "    if (n%in%c('Conv', \"LSTM\", \"MLP\")){\n",
    "        \"joint model + MFCC\"\n",
    "    } else if (n%in%c('Conv_mid', \"LSTM_mid\", \"MLP_mid\")){\n",
    "        \"joint model + Mel Spectrum\"\n",
    "    } else{\n",
    "        \"Independent + MFCC\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(to_plot)\n",
    "\n",
    "to_plot$metric <- factor(to_plot$metric)\n",
    "\n",
    "p <- ggplot(to_plot, aes(x=factor(algo), y=score, color=family, fill=family)) +\n",
    "    geom_bar(stat='identity') +\n",
    "    facet_grid(metric~family, scales='free')\n",
    "print(p)\n",
    "\n",
    "ggsave('results_guitar.pdf', p, width=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model(model, X_train, X_test):\n",
    "    \n",
    "    try:\n",
    "        # Prepares input and output data\n",
    "        X_mean = np.mean(X_train, axis=(0))\n",
    "        X_sd = np.std(X_train, axis=(0))\n",
    "\n",
    "        X_train = (X_train - X_mean) / X_sd\n",
    "        X_test  = (X_test - X_mean) / X_sd\n",
    "\n",
    "        Y_dict_train = format_output(Y_train)\n",
    "        Y_dict_test  = format_output(Y_test)\n",
    "\n",
    "        # Trains the model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        hist = model.fit(X_train, Y_dict_train, \n",
    "                  epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE,\n",
    "                 validation_split = VALIDATION, callbacks=[early_stopping])\n",
    "\n",
    "        # Makes predictions\n",
    "        pred = model.predict(X_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "\n",
    "        # Gets validation and test scores (for each metric)\n",
    "        Y = {}\n",
    "        for j, Y_pred in enumerate(pred):\n",
    "            t_name = Y_info.keys()[j]\n",
    "            t_infos = Y_info[t_name]\n",
    "            truth = Y_test[t_name]\n",
    "            print t_name\n",
    "\n",
    "            if t_infos['type'] == 'float64':\n",
    "                test_metric = 'reg'\n",
    "                Y_pred = Y_pred.flatten()\n",
    "                y_pred = Y_pred * t_infos['sd'] + t_infos['mean']\n",
    "                y_pred = y_pred.tolist()\n",
    "\n",
    "            elif t_infos['type'] == 'object':\n",
    "                test_metric = 'class'\n",
    "                y_i = np.argmax(Y_pred, axis=1)\n",
    "                y_pred = [t_infos['int2char'][y] for y in y_i]\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "\n",
    "            Y[t_name] = (y_pred, truth)\n",
    "            \n",
    "    except:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "        return\n",
    "    \n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in_dim = X_train_mfcc.shape[1:]\n",
    "model = modLSTM(128, 2, 0, in_dim)\n",
    "out = show_model(model, X_train_mfcc, X_test_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
